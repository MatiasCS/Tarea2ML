{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/vowel.train\"\n",
    "test_data_url = \"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/vowel.test\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "train_df = pd.DataFrame.from_csv('train_data.csv',header=0,index_col=0)\n",
    "test_df = pd.DataFrame.from_csv('test_data.csv',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = train_df.ix[:,'x.1':'x.10'].values\n",
    "y = train_df.ix[:,'y'].values\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.d Reducción de dimensionalidad via LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.lda import LDA\n",
    "#nuevo\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#sklearn_lda = LDA(n_components=2)\n",
    "sklearn_lda = KernelPCA(n_components=2,kernel=\"rbf\", fit_inverse_transform=True)\n",
    "Xred_lda = sklearn_lda.fit_transform(X_std,y)\n",
    "cmap = plt.cm.get_cmap('RdYlGn')\n",
    "mclasses=(1,2,3,4,5,6,7,8,9,10,11)\n",
    "mcolors = [cmap(i) for i in np.linspace(0,1,11)]\n",
    "plt.figure(figsize=(12, 8))\n",
    "for lab, col in zip(mclasses,mcolors):\n",
    "    plt.scatter(Xred_lda[y==lab, 0],Xred_lda[y==lab, 1],label=lab,c=col)\n",
    "plt.xlabel('LDA/Fisher Direction 1')\n",
    "plt.ylabel('LDA/Fisher Direction 2')\n",
    "leg = plt.legend(loc='upper right', fancybox=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar LDA para reducir dimensiones se proyecta la data en la dirección de máxima varianza entre las clases del dataset. En general cada reducción busca, en el mejor de los casos disminuir el ruido del modelo y además, eliminar datos que puedan ser redundantes dentro del problema estudiado. Es importante destacar LDA por definición si utiliza información de las etiquetas de los ejemplos para realizar la reducción de dimensionalidad. Por lo que se obtienen mejores resultados que al aplicar PCA.\n",
    "\n",
    "TODO:  incluir la formula de LDA buscar porque se llama fisher direction\n",
    "\n",
    "Al comparar los gráficos obtenidos mediante PCA y LDA, se observa que con el primero se obtienen peores resultados, puesto que no se logran identificar zonas claras en las que predomine una clase. En cambio con LDA se obtiene resultados mejores.\n",
    "\n",
    "La clase 5 con la clase 6 tienden aun a estar un poco superpuestas, al igual que con la clase 9 y 10. Por otro lado se observa que la clase 1 sigue dispersa al proyectarlas en las componentes encontradas por LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
