{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x.1</th>\n",
       "      <th>x.2</th>\n",
       "      <th>x.3</th>\n",
       "      <th>x.4</th>\n",
       "      <th>x.5</th>\n",
       "      <th>x.6</th>\n",
       "      <th>x.7</th>\n",
       "      <th>x.8</th>\n",
       "      <th>x.9</th>\n",
       "      <th>x.10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>-3.246078</td>\n",
       "      <td>2.049102</td>\n",
       "      <td>-0.576076</td>\n",
       "      <td>0.504626</td>\n",
       "      <td>-0.210089</td>\n",
       "      <td>0.681998</td>\n",
       "      <td>-0.029327</td>\n",
       "      <td>0.244162</td>\n",
       "      <td>-0.342820</td>\n",
       "      <td>-0.056221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.165706</td>\n",
       "      <td>0.753377</td>\n",
       "      <td>1.170402</td>\n",
       "      <td>0.671069</td>\n",
       "      <td>0.748236</td>\n",
       "      <td>0.578353</td>\n",
       "      <td>0.544476</td>\n",
       "      <td>0.440483</td>\n",
       "      <td>0.532523</td>\n",
       "      <td>0.505557</td>\n",
       "      <td>0.650602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.982000</td>\n",
       "      <td>-1.074000</td>\n",
       "      <td>-2.091000</td>\n",
       "      <td>-1.044000</td>\n",
       "      <td>-1.733000</td>\n",
       "      <td>-0.405000</td>\n",
       "      <td>-1.282000</td>\n",
       "      <td>-0.949000</td>\n",
       "      <td>-1.409000</td>\n",
       "      <td>-1.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>-3.855750</td>\n",
       "      <td>1.194000</td>\n",
       "      <td>-1.037000</td>\n",
       "      <td>-0.049250</td>\n",
       "      <td>-0.612000</td>\n",
       "      <td>0.278250</td>\n",
       "      <td>-0.310250</td>\n",
       "      <td>-0.167750</td>\n",
       "      <td>-0.721750</td>\n",
       "      <td>-0.564500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>-3.220000</td>\n",
       "      <td>2.101500</td>\n",
       "      <td>-0.621000</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>-0.181500</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>-0.358000</td>\n",
       "      <td>-0.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>-2.706500</td>\n",
       "      <td>2.985000</td>\n",
       "      <td>-0.181000</td>\n",
       "      <td>0.960750</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>1.038500</td>\n",
       "      <td>0.245750</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.594750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>-1.093000</td>\n",
       "      <td>4.314000</td>\n",
       "      <td>1.431000</td>\n",
       "      <td>2.377000</td>\n",
       "      <td>1.114000</td>\n",
       "      <td>2.108000</td>\n",
       "      <td>1.209000</td>\n",
       "      <td>2.039000</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.294000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                y         x.1         x.2         x.3         x.4         x.5  \\\n",
       "count  462.000000  462.000000  462.000000  462.000000  462.000000  462.000000   \n",
       "mean     6.000000   -3.246078    2.049102   -0.576076    0.504626   -0.210089   \n",
       "std      3.165706    0.753377    1.170402    0.671069    0.748236    0.578353   \n",
       "min      1.000000   -4.982000   -1.074000   -2.091000   -1.044000   -1.733000   \n",
       "25%      3.000000   -3.855750    1.194000   -1.037000   -0.049250   -0.612000   \n",
       "50%      6.000000   -3.220000    2.101500   -0.621000    0.418500   -0.181500   \n",
       "75%      9.000000   -2.706500    2.985000   -0.181000    0.960750    0.199000   \n",
       "max     11.000000   -1.093000    4.314000    1.431000    2.377000    1.114000   \n",
       "\n",
       "              x.6         x.7         x.8         x.9        x.10  \n",
       "count  462.000000  462.000000  462.000000  462.000000  462.000000  \n",
       "mean     0.681998   -0.029327    0.244162   -0.342820   -0.056221  \n",
       "std      0.544476    0.440483    0.532523    0.505557    0.650602  \n",
       "min     -0.405000   -1.282000   -0.949000   -1.409000   -1.241000  \n",
       "25%      0.278250   -0.310250   -0.167750   -0.721750   -0.564500  \n",
       "50%      0.593000    0.005500    0.245000   -0.358000   -0.257500  \n",
       "75%      1.038500    0.245750    0.651500    0.019500    0.594750  \n",
       "max      2.108000    1.209000    2.039000    0.757000    1.294000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/vowel.train\"\n",
    "test_data_url = \"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/vowel.test\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "train_df = pd.DataFrame.from_csv('train_data.csv',header=0,index_col=0)\n",
    "test_df = pd.DataFrame.from_csv('test_data.csv',header=0,index_col=0)\n",
    "train_df.head()\n",
    "test_df.tail()\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = train_df.ix[:,'x.1':'x.10'].values\n",
    "y = train_df.ix[:,'y'].values\n",
    "X_std = StandardScaler().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a 1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección de la tarea se trabajó con un set de datos que representa una colección de sonidos fonéticos. A cada uno de los sonidos se le asocia una vocal del idioma Ingles, lo cual nos da un total de 11 clases posibles. Los autores entregan aproximación en ascii de cada vocal, junto con la palabra en la que se pronunció. Esto se ilustra en la siguiente tabla: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Vocal</th>\n",
    "    <th>Palabra</th>\n",
    "    <th>Vocal</th>\n",
    "    <th>Palabra</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>i</td>\n",
    "    <td>heed</td>\n",
    "    <td>o</td>\n",
    "    <td>hod</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>I</td>\n",
    "    <td>hid</td>\n",
    "    <td>C:</td>\n",
    "    <td>hoard</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>E</td>\n",
    "    <td>head</td>\n",
    "    <td>U</td>\n",
    "    <td>hood</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>A</td>\n",
    "    <td>had</td>\n",
    "    <td>u:</td>\n",
    "    <td>who'd</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>a:</td>\n",
    "    <td>hard</td>\n",
    "    <td>3:</td>\n",
    "    <td>heard</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Y</td>\n",
    "    <td>hud</td>\n",
    "    <td></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Mediante la participación de 15 personas que pronunciaban las palabras se orginaron dos sets de datos. El training set consta de 528 ejemplos, mientras que el testing set consta de 462. Cada ejemplo consta de 11 atributos, siendo uno de ellos a la clase a la cual pertenece dicho ejemplo, mientras que los otros 10 son generados siguiendo la propuesta de Rabiner y Schaffer para el procesamiento del habla.\n",
    "\n",
    "Por escribir: importancia de la normalización del dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c Reducción de dimensionalidad via PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sklearn_pca = PCA(n_components=2)\n",
    "Xred_pca = sklearn_pca.fit_transform(X_std)\n",
    "cmap = plt.cm.get_cmap('RdYlGn')\n",
    "mclasses=(1,2,3,4,5,6,7,8,9,10,11)\n",
    "mcolors = [cmap(i) for i in np.linspace(0,1,11)]\n",
    "plt.figure(figsize=(12, 8))\n",
    "for lab, col in zip(mclasses,mcolors):\n",
    "    plt.scatter(Xred_pca[y==lab, 0],Xred_pca[y==lab, 1],label=lab,c=col)\n",
    "plt.title('PCA reduction vowel training set')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "leg = plt.legend(loc='upper right', fancybox=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al utilizar PCA para reducir dimensiones se proyecta la data en la dirección de máxima varianza del dataset. En general cada reducción busca, en el mejor de los casos disminuir el ruido del modelo y además, eliminar datos que puedan ser redundantes dentro del problema estudiado. Es importante destacar PCA por definición no utiliza informacion de las etiquetas de los ejemplos para realizar la reducción de dimensionalidad. Por lo que no siempre se obtienen los resultados buscados con este método.\n",
    "\n",
    "TODO:  incluir la formula de PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
